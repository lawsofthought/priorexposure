\documentclass{slides}

\title[MCMC]{Mixture Models}
\author[Andrews]{Mark Andrews \& Thom Baguley}

\begin{document}
{
	\begin{frame}
		\titlepage
	\end{frame}
}


\begin{frame}
	\frametitle{Mixture models}
	\begin{itemize}

		\item Given a set of observed variables $y_1, y_2 \ldots y_n$,
			it is common to model them as distributed according to
			a parameteric density function density function
			$\Prob{y \given \theta}$ whose parameter $\theta$ are
			unknown.

		\item For example, we could model $y_1, y_2 \ldots y_n$ as 
			\[
				y_i \sim N(\mu, \sigma^2),\quad\text{for $i \in {1 \ldots n}$.}
			\]
		where $\mu$ and $\sigma$ are unknown.

	\item We could, however, models $y_1, y_2 \ldots y_n$ as distributed according to a finite \emph{mixture} of parametric density functions.
		For example, 
		\[
			y_i \sim \sum_{k=1}^K N(\mu_k, \sigma_k) \Prob{\mu_k, \sigma_k}, \quad\text{for $i \in {1 \ldots n}$.}
		\]
	

	\end{itemize}
	\end{frame}

	\begin{frame}
	\frametitle{Mixture Models}
	\noindent A mixture of Gaussians density function.
	\vspace{-2\baselineskip}
	\begin{center}
	\input{figs/mog1}
	\end{center}
	\end{frame}

	\begin{frame}
	\frametitle{Mixture Models}
	\noindent A mixture of Gaussians density function with component distributions shown in red.
	\vspace{-2\baselineskip}
	\begin{center}
	\input{figs/mog2}
	\end{center}
	\end{frame}

	\begin{frame}
	\frametitle{Mixture models as latent variable models}
	\begin{itemize}

	\item If $y_1, y_2 \ldots y_n$ are modelled as 
		\[
			y_i \sim \sum_{k=1}^K N(\mu_k, \sigma_k) \Prob{\mu_k, \sigma_k}, \quad\text{for $i \in {1 \ldots n}$,}
		\]
		this is equivalent to each $y_i$ corresponding to a discrete latent variable $x_i \in \{1, 2 \ldots K\}$ and
		\[
			y_i \sim N(\mu_{[x_i]}, \sigma_{[x_i]}),\quad\text{for $i \in {1 \ldots n}$.}
		\]
		and
		\[
			x_i \sim P(\pi_k), \quad\text{where $\pi_k = \Prob{\mu_k, \sigma_k}$}.
		\]
	
	\end{itemize}
	\end{frame}


	\begin{frame}
	\frametitle{Mixture models as latent variable models}
	\begin{itemize}
	\item A general way of writing a mixture model is 
		\[
			\Prob{y_i} \sim \sum_{k=1}^K \Prob{y \given \theta, x_i=k}\Prob{x_i=k\given\pi}, \quad\text{for $i \in {1 \ldots n}$.}
		\]
	\item This is equivalent to the following generative model: \\
		For $i \in \{1, 2 \ldots n\}$,
		\begin{align*}
			x_i &\sim \mathrm{Categorical}(\pi),\\
			y_i &\sim \Prob{y\given \theta_{[x_i]}}
		\end{align*}
	\end{itemize}
	\end{frame}


	\begin{frame}
	\frametitle{Bayesian inference in mixture models as latent variable models}
	\begin{itemize}
	\item Given the general way form
		\[
			\Prob{y_i} \sim \sum_{k=1}^K \Prob{y \given \theta, x_i=k}\Prob{x_i=k\given\pi}, \quad\text{for $i \in {1 \ldots n}$.}
		\]
		the observed data are\[ D = y_1, y_2 \ldots y_n,\] 
		while the parameters 
		\[\theta = \theta_1, \theta_2 \ldots \theta_K\]
		and the values of the latent variables \[x_1, x_2 \ldots x_n\] are unknown.
	\end{itemize}
	\end{frame}


	\begin{frame}
	\frametitle{Bayesian inference in mixture models as latent variable models}
	\begin{itemize}
		\item The full posterior is \[
		\Prob{\theta, \pi, x_{1:n} \given y_{1:n}} \propto \Prob{\pi}\Prob{\theta}\prod_{i=1}^n \Prob{y \given \theta, x_i=k}\Prob{x_i=k\given\pi}
		\]
	\item We can often sample from this using a Gibbs sampler. For example, the conditional distributions 
		\[
			\Prob{x_i = k \given \theta, \pi, y_{1:n}} \propto 
			\Prob{y_i \given x_i=k, \theta}\Prob{x_i = k \given \pi}
		\]
		and \[
			\Prob{\theta_k \given x_{1:n}, y_{1:n}, \pi} \propto \Prob{\theta_k} \prod_{\{i \colon x_i=k\}}\Prob{y_i \given \theta_k}
		\]
		and \[
			\Prob{\pi\given x_{1:n}} \propto \Prob{x_{1:n}\given \pi}\Prob{\pi}
		\]
		are often analytically tractable.
	\end{itemize}
	\end{frame}


\begin{frame}
\frametitle{Example: Zero-Inflated Poisson distribution}
\begin{itemize}
\item A zero inflated Poisson model is a simple two component mixture model.
\item According to this model, the observed $y_1, y_2 \ldots y_n$ are assumed to generated by
\begin{align*}
y_i &\sim \begin{cases} \textrm{Poisson}(\lambda)\quad &\text{if $x_i=0$},\\ 0, \quad &\text{if $x_i=1$} \end{cases},\\
x_i &\sim \textrm{Bernoulli}(\pi).
\end{align*}
\end{itemize}
\end{frame}
	\begin{frame}
	\frametitle{Poisson Distribution}
	\noindent A sample from a Poisson distribution with $\lambda=5$.
	\vspace{-2\baselineskip}
	\begin{center}
	\input{figs/poissonplot1.tex}
	\end{center}
	\end{frame}


	\begin{frame}
	\frametitle{Zero-Inflated Poisson Distribution}
	\noindent A sample from a Zero-inflated Poisson model, with Poisson distribution component with $\lambda=5$, and probability of zero-model being $.25$.
	\vspace{-2\baselineskip}
	\begin{center}
	\input{figs/poissonplot2.tex}
	\end{center}
	\end{frame}



\begin{frame}
	\frametitle{Dirichlet Process mixture models}
	\begin{itemize}
		\item Dirichlet Process mixture model can overcome the problem of choosing $K$, the number of component distributions.
		\item In a Dirichlet Process mixture model, our generative model is
		\\For $i \in \{1, 2 \ldots n\}$,
		\begin{align*}
			\theta_i &\sim \mathrm{DP}(\alpha G_0),\\
			y_i &\sim \Prob{y\given \theta_i}
		\end{align*}
		which is equivalent to 
		\[
			\Prob{y_i} \sim \sum_{k=1}^\infty \Prob{y \given \theta, x_i=k}\Prob{x_i=k\given\pi}, \quad\text{for $i \in {1 \ldots n}$.}
		\]
	

	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Dirichlet Process mixture models}
	\begin{itemize}
		\item A Dirichlet Process with \emph{base measure} $G_0$ and \emph{concentration} parameter $\alpha$ can be defined by followng density:
			\[
				f(\theta) = \sum_{k=1}^\infty \pi_k \delta_{\theta_k}(\theta)
			\]
			where each $\pi_k$ is drawn from a \emph{stick-breaking} prior with parameter $\alpha$, each $\theta_k$ is drawn $G_0$, and 
		\[
			\delta_{\theta_k}(\theta)
		\]
		is an function over $\theta$ that takes the value of $1$ if $\theta=\theta_k$ and $0$ otherwise.
		\item In other words, a Dirchlet Process $f(\theta)$ is an infinite distribution of \emph{spikes} on the $\theta$ parameter space.


	\end{itemize}
\end{frame}



\end{document}
